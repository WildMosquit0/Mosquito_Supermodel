{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated video saved as: /home/wildmosquit0/workspace/data/images/Joni_reptilians/rept.mp4\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "\n",
    "# Define YOLO class labels (using COCO dataset classes as an example)\n",
    "YOLO_LABELS = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\",\n",
    "    \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\n",
    "    \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\",\n",
    "    \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
    "    \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\",\n",
    "    \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\",\n",
    "    \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\",\n",
    "    \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\",\n",
    "    \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\",\n",
    "    \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\",\n",
    "    \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "]\n",
    "\n",
    "def create_annotated_video(\n",
    "    df: pd.DataFrame, \n",
    "    labels_to_show: List[int],\n",
    "    input_video_path: str,\n",
    "    output_video_path: str,\n",
    "    confidence_threshold: float = 0.3\n",
    "):\n",
    "    \"\"\"\n",
    "    Creates an annotated video showing bounding boxes and labels for the specified detections.\n",
    "    For detections with the \"person\" label, in the last 10 frames only the most centered detection \n",
    "    (as determined by the 'track_id') is annotated as \"Reptilian 1.0\" with a dark green bounding box.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame with columns:\n",
    "            - image_idx: index of the frame\n",
    "            - label: class label (as integer corresponding to an index in YOLO_LABELS)\n",
    "            - confidence: detection confidence\n",
    "            - x, y: bounding box center (in pixel coordinates)\n",
    "            - w, h: bounding box width and height\n",
    "            - track_id: unique identifier for tracking a given object across frames.\n",
    "        labels_to_show (List[int]): List of label indices to display in the video.\n",
    "        input_video_path (str): Path to the input video file.\n",
    "        output_video_path (str): Path to the output (annotated) video file.\n",
    "        confidence_threshold (float): Only draw bounding boxes with confidence above this value.\n",
    "    \"\"\"\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Could not open video file: {input_video_path}\")\n",
    "        return\n",
    "\n",
    "    # Video properties\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps    = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    \n",
    "    # Video writer setup\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    # Pre-filter DataFrame: use only detections above the confidence threshold and within specified labels.\n",
    "    df_filtered = df[(df['confidence'] > confidence_threshold) & (df['label'].isin(labels_to_show))]\n",
    "    grouped = df_filtered.groupby(\"image_idx\")\n",
    "    \n",
    "    frame_idx = 0  \n",
    "    last_frame = None  # to store the final annotated frame\n",
    "    \n",
    "    # Variable to hold the chosen track_id for \"Reptilian\"\n",
    "    reptilian_track_id = None\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Optional extra break if frame_idx exceeds total_frames\n",
    "        if total_frames > 0 and frame_idx >= total_frames:\n",
    "            break\n",
    "\n",
    "        # Get detections for the current frame (if any)\n",
    "        if frame_idx in grouped.groups:\n",
    "            frame_df = grouped.get_group(frame_idx)\n",
    "        else:\n",
    "            frame_df = pd.DataFrame(columns=df.columns)\n",
    "        \n",
    "        # In the last 10 frames, if we haven't chosen a reptilian track yet, pick the most centered \"person\"\n",
    "        if total_frames > 0 and frame_idx >= total_frames - 10:\n",
    "            if reptilian_track_id is None:\n",
    "                person_detections = frame_df[frame_df['label'] == 0]\n",
    "                if not person_detections.empty:\n",
    "                    frame_center_x = frame.shape[1] // 2\n",
    "                    frame_center_y = frame.shape[0] // 2\n",
    "                    min_dist = float('inf')\n",
    "                    for _, row in person_detections.iterrows():\n",
    "                        dist = ((row['x'] - frame_center_x) ** 2 + (row['y'] - frame_center_y) ** 2) ** 0.5\n",
    "                        if dist < min_dist:\n",
    "                            min_dist = dist\n",
    "                            reptilian_track_id = row['track_id']\n",
    "        \n",
    "        # Process each detection in this frame\n",
    "        for idx, row in frame_df.iterrows():\n",
    "            label_idx = int(row['label'])\n",
    "            conf = row['confidence']\n",
    "            # Retrieve label name from YOLO_LABELS (if index valid)\n",
    "            label_name = YOLO_LABELS[label_idx] if label_idx < len(YOLO_LABELS) else str(label_idx)\n",
    "            \n",
    "            # Default: use a hash-based color for each label\n",
    "            color = ((label_idx * 37) % 256, (label_idx * 67) % 256, (label_idx * 97) % 256)\n",
    "            text = f\"{label_name} {conf:.2f}\"\n",
    "            \n",
    "            # For the last 10 frames, if the detection is a \"person\" and its track_id matches reptilian_track_id, override the text and color.\n",
    "            if total_frames > 0 and frame_idx >= total_frames - 10 and label_idx == 0:\n",
    "                if reptilian_track_id is not None and row['track_id'] == reptilian_track_id:\n",
    "                    text = \"Reptilian 1.0\"\n",
    "                    color = (0, 128, 0)  # Dark green\n",
    "            \n",
    "            x_center = row['x']\n",
    "            y_center = row['y']\n",
    "            w_box = row['w']\n",
    "            h_box = row['h']\n",
    "            \n",
    "            # Calculate bounding box corners\n",
    "            x1 = int(x_center - w_box / 2)\n",
    "            y1 = int(y_center - h_box / 2)\n",
    "            x2 = int(x_center + w_box / 2)\n",
    "            y2 = int(y_center + h_box / 2)\n",
    "            \n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            \n",
    "            # Prepare text with background rectangle for improved visibility\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "            font_scale = 1\n",
    "            thickness = 1\n",
    "            (text_width, text_height), baseline = cv2.getTextSize(text, font, font_scale, thickness)\n",
    "            y_label = max(0, y1 - 5)\n",
    "            rect_x1 = max(x1, 0)\n",
    "            rect_y1 = max(y_label - text_height - baseline, 0)\n",
    "            rect_x2 = min(x1 + text_width, frame.shape[1])\n",
    "            rect_y2 = min(y_label, frame.shape[0])\n",
    "            \n",
    "            cv2.rectangle(frame, (rect_x1, rect_y1), (rect_x2, rect_y2), color, cv2.FILLED)\n",
    "            cv2.putText(frame, text, (rect_x1, rect_y2 - baseline), font, font_scale, (255, 255, 255), thickness, cv2.LINE_AA)\n",
    "            \n",
    "            \n",
    "        out.write(frame)\n",
    "        last_frame = frame.copy()\n",
    "        frame_idx += 1\n",
    "        \n",
    "        # Optional: Allow manual exit (e.g., when processing live streams)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Annotated video saved as: {output_video_path}\")\n",
    "    \n",
    "    # Display the last annotated frame\n",
    "    if last_frame is not None:\n",
    "        cv2.imshow(\"Last Annotated Frame\", last_frame)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage: create a dummy DataFrame for demonstration.\n",
    "    # In practice, replace this with your actual detections.\n",
    "    columns = [\"image_idx\", \"label\", \"confidence\", \"x\", \"y\", \"w\", \"h\"]\n",
    "    # Create dummy data for a 100-frame video; every frame has one detection.\n",
    "    # For demonstration, label 0 (\"person\") is used for every frame.\n",
    "\n",
    "    df = pd.read_csv(\"/home/wildmosquit0/workspace/data/images/Joni_reptilians/track/results.csv\")\n",
    "    \n",
    "    # Specify which labels to show (here, only label 0 corresponding to \"person\")\n",
    "    labels_to_show = list(range(len(YOLO_LABELS)))\n",
    "    \n",
    "    # Replace these paths with your actual video paths.\n",
    "    input_video_path = \"/home/wildmosquit0/workspace/data/images/Joni_reptilians/baylor.mp4\"\n",
    "    output_video_path = \"/home/wildmosquit0/workspace/data/images/Joni_reptilians/rept.mp4\"\n",
    "    \n",
    "    create_annotated_video(df, labels_to_show, input_video_path, output_video_path, confidence_threshold=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotated video saved as: /home/wildmosquit0/workspace/data/images/Joni_reptilians/rept.mp4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# First load the CSV file into a DataFrame\n",
    "\n",
    "\n",
    "# Then call your function, passing the actual DataFrame (df)\n",
    "create_annotated_video(\n",
    "    df,                       # <-- pass the DataFrame, not the CSV path\n",
    "    labels_to_show=YOLO_LABELS,                        # class_label\n",
    "    input_video_path = \"/home/wildmosquit0/workspace/data/images/Joni_reptilians/baylor.mp4\", \n",
    "    output_video_path = \"/home/wildmosquit0/workspace/data/images/Joni_reptilians/rept.mp4\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLO_LABELS = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorbike\", \"aeroplane\", \"bus\", \"train\", \"truck\",\n",
    "    \"boat\", \"traffic light\", \"fire hydrant\", \"stop sign\", \"parking meter\", \"bench\",\n",
    "    \"bird\", \"cat\", \"dog\", \"horse\", \"sheep\", \"cow\", \"elephant\", \"bear\", \"zebra\",\n",
    "    \"giraffe\", \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\", \"frisbee\",\n",
    "    \"skis\", \"snowboard\", \"sports ball\", \"kite\", \"baseball bat\", \"baseball glove\",\n",
    "    \"skateboard\", \"surfboard\", \"tennis racket\", \"bottle\", \"wine glass\", \"cup\", \"fork\",\n",
    "    \"knife\", \"spoon\", \"bowl\", \"banana\", \"apple\", \"sandwich\", \"orange\", \"broccoli\",\n",
    "    \"carrot\", \"hot dog\", \"pizza\", \"donut\", \"cake\", \"chair\", \"sofa\", \"pottedplant\",\n",
    "    \"bed\", \"diningtable\", \"toilet\", \"tvmonitor\", \"laptop\", \"mouse\", \"remote\",\n",
    "    \"keyboard\", \"cell phone\", \"microwave\", \"oven\", \"toaster\", \"sink\", \"refrigerator\",\n",
    "    \"book\", \"clock\", \"vase\", \"scissors\", \"teddy bear\", \"hair drier\", \"toothbrush\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'unique'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mYOLO_LABELS\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'unique'"
     ]
    }
   ],
   "source": [
    "YOLO_LABELS.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sp39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
