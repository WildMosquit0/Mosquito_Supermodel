{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Be sure current working directory is 'demo/' folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Import required functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi.slicing import slice_coco\n",
    "from sahi.utils.file import load_json\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import shutil\n",
    "import multiprocessing\n",
    "import tempfile\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_train_source = '/home/bohbot/ultralytics/datasets/mos/all_mos_new/images/train'\n",
    "images_val_source = '/home/bohbot/ultralytics/datasets/mos/all_mos_new/images/val'\n",
    "labels_train_source = '/home/bohbot/ultralytics/datasets/mos/all_mos_new/labels/train'\n",
    "labels_val_source = '/home/bohbot/ultralytics/datasets/mos/all_mos_new/labels/val'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_paths = (\n",
    "    [image_path for image_path in glob.iglob(f\"{images_train_source}/*\") \n",
    "     if image_path.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff'))] +\n",
    "    [image_path for image_path in glob.iglob(f\"{images_val_source}/*\") \n",
    "     if image_path.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff'))]\n",
    ")\n",
    "\n",
    "all_annots_paths = (\n",
    "    [annot_path for annot_path in glob.iglob(f\"{labels_train_source}/*\") \n",
    "     if annot_path.endswith('txt')] +\n",
    "    [annot_path for annot_path in glob.iglob(f\"{labels_val_source}/*\") \n",
    "     if annot_path.endswith('txt')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annot_dict = {\n",
    "    os.path.splitext(os.path.basename(annot_path))[0]: annot_path\n",
    "    for annot_path in all_annots_paths if 'test' not in annot_path\n",
    "}\n",
    "\n",
    "image_annot_paths = [\n",
    "    (image_path, annot_dict[os.path.splitext(os.path.basename(image_path))[0]])\n",
    "    for image_path in all_image_paths\n",
    "    if os.path.splitext(os.path.basename(image_path))[0] in annot_dict\n",
    "]\n",
    "\n",
    "len(image_annot_paths), image_annot_paths[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_paths, large_files_dir):\n",
    "    try:\n",
    "        image_path, annot_path = file_paths\n",
    "\n",
    "        if not os.path.exists(image_path) or not os.path.exists(annot_path):\n",
    "            print(f\"Missing file: {image_path} or {annot_path}\")\n",
    "            return\n",
    "\n",
    "        with Image.open(image_path) as img:\n",
    "            width, height = img.size\n",
    "            if width > 1000 or height > 1000:\n",
    "                shutil.copy(image_path, large_files_dir)\n",
    "                shutil.copy(annot_path, large_files_dir)\n",
    "            else:\n",
    "                return\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_paths}: {e}\")\n",
    "\n",
    "\n",
    "def copy_list(image_annot_paths, large_files_dir):\n",
    "    os.makedirs(large_files_dir, exist_ok=True)\n",
    "    copied_count = 0\n",
    "    for file_paths in tqdm(image_annot_paths):\n",
    "        process_file(file_paths, large_files_dir)\n",
    "        copied_count += 1\n",
    "    print(f\"Total files copied: {copied_count}\")\n",
    "\n",
    "def copy_large_images(image_annot_paths, large_files_dir):\n",
    "    copied_count = 0\n",
    "    with ThreadPoolExecutor(max_workers=multiprocessing.cpu_count()-2) as executor:\n",
    "        results = list(executor.map(\n",
    "            lambda x: process_file(x, large_files_dir),\n",
    "            image_annot_paths\n",
    "        ))\n",
    "    print(f\"Total files copied: {copied_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory_or_paths = image_annot_paths # folder with both images and txt files\n",
    "# large_files_directory = '/home/bohbot/Evyatar/git/crop_sahi/large_files' # copies only the large images to a different folder\n",
    "batch_output_directory = '/home/bohbot/Evyatar/git/crop_sahi/large_images/images' # where to save the crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_list(image_annot_paths, batch_output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. convert yolo annotations to coco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(args):\n",
    "    image_filename, images_dir, yolo_annotations_dir = args[:3]\n",
    "    category_mapping = args[3]\n",
    "\n",
    "    image_path = os.path.join(images_dir, image_filename)\n",
    "    annotation_filename = os.path.splitext(image_filename)[0] + \".txt\"\n",
    "    annotation_path = os.path.join(yolo_annotations_dir, annotation_filename)\n",
    "\n",
    "    if not os.path.exists(annotation_path):\n",
    "        return None, None, None\n",
    "\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        return None, None, None\n",
    "\n",
    "    height, width, _ = image.shape\n",
    "    image_id = hash(image_filename)\n",
    "\n",
    "    image_data = {\n",
    "        \"id\": image_id,\n",
    "        \"file_name\": image_filename,\n",
    "        \"height\": height,\n",
    "        \"width\": width\n",
    "    }\n",
    "\n",
    "    annotations = []\n",
    "    categories = []\n",
    "\n",
    "    with open(annotation_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line_data = line.strip().split()\n",
    "            if len(line_data) < 5:\n",
    "                continue\n",
    "\n",
    "            category_id = int(line_data[0])\n",
    "            x_center, y_center, bbox_width, bbox_height = map(float, line_data[1:])\n",
    "\n",
    "            if category_id not in category_mapping:\n",
    "                category_mapping[category_id] = f\"category_{category_id}\"\n",
    "                categories.append({\n",
    "                    \"id\": category_id,\n",
    "                    \"name\": category_mapping[category_id],\n",
    "                    \"supercategory\": \"none\"\n",
    "                })\n",
    "\n",
    "            x_min = (x_center - bbox_width / 2) * width\n",
    "            y_min = (y_center - bbox_height / 2) * height\n",
    "            bbox_width *= width\n",
    "            bbox_height *= height\n",
    "\n",
    "            annotations.append({\n",
    "                \"id\": len(annotations) + 1,\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category_id,\n",
    "                \"bbox\": [x_min, y_min, bbox_width, bbox_height],\n",
    "                \"area\": bbox_width * bbox_height,\n",
    "                \"iscrowd\": 0\n",
    "            })\n",
    "\n",
    "    return annotations, image_data, categories\n",
    "\n",
    "\n",
    "def convert_yolo_to_coco(yolo_annotations_dir, images_dir, output_json_path):\n",
    "    os.makedirs(output_json_path, exist_ok=True)\n",
    "    coco_format = {\"images\": [], \"categories\": [], \"annotations\": []}\n",
    "    category_mapping = {}\n",
    "\n",
    "    image_filenames = [\n",
    "        f for f in os.listdir(images_dir)\n",
    "        if f.lower().endswith((\".jpg\", \".png\", \".jpeg\"))\n",
    "    ]\n",
    "\n",
    "    args = [\n",
    "        (image_filename, images_dir, yolo_annotations_dir, category_mapping)\n",
    "        for image_filename in image_filenames\n",
    "    ]\n",
    "\n",
    "    with Pool(processes=os.cpu_count()-2) as pool:\n",
    "        results = list(\n",
    "            tqdm(pool.imap(process_image, args), total=len(args), desc=\"Processing images\")\n",
    "        )\n",
    "\n",
    "    for annotations, image_data, categories in results:\n",
    "        if annotations and image_data:\n",
    "            coco_format[\"annotations\"].extend(annotations)\n",
    "            coco_format[\"images\"].append(image_data)\n",
    "        if categories:\n",
    "            coco_format[\"categories\"].extend(categories)\n",
    "\n",
    "    category_ids = set()\n",
    "    unique_categories = []\n",
    "    for category in coco_format[\"categories\"]:\n",
    "        if category[\"id\"] not in category_ids:\n",
    "            category_ids.add(category[\"id\"])\n",
    "            unique_categories.append(category)\n",
    "    coco_format[\"categories\"] = unique_categories\n",
    "\n",
    "    with open(f\"{output_json_path}/coco_annotations.json\", \"w\") as json_file:\n",
    "        json.dump(coco_format, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_annotations_path = f\"{batch_output_directory.replace('/images','/coco')}\"\n",
    "sliced_images_path = f\"{batch_output_directory.replace('/images','/sliced')}\"\n",
    "temp_image_dir = f\"{batch_output_directory.replace('/images','/temp_processing')}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_yolo_to_coco(batch_output_directory, batch_output_directory, coco_annotations_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Slicing COCO Dataset into Grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To slice a COCO dataset annoations an images, we need to specify slice parameters. In this example we will ice images into 256x256 grids overlap ratio of 0.2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_files = len(glob.glob(f\"{batch_output_directory}/*txt\"))\n",
    "n_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = n_files\n",
    "json_out =f\"{coco_annotations_path}/coco_annotations.json\"\n",
    "sliced_dir = sliced_images_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 6915/9779 [08:30<03:06, 15.36it/s]"
     ]
    }
   ],
   "source": [
    "from dask import bag as db\n",
    "\n",
    "def process_file(image_file):\n",
    "    try:\n",
    "        slice_coco(\n",
    "            coco_annotation_file_path=json_out,\n",
    "            image_dir=batch_output_directory,\n",
    "            output_coco_annotation_file_name=\"sliced_annotations\",\n",
    "            ignore_negative_samples=False,\n",
    "            output_dir=sliced_dir,\n",
    "            slice_height=640,\n",
    "            slice_width=640,\n",
    "            overlap_height_ratio=0.2,\n",
    "            overlap_width_ratio=0.2,\n",
    "            min_area_ratio=0.1,\n",
    "            verbose=False\n",
    "        )\n",
    "    except Exception:\n",
    "        return image_file\n",
    "    return None\n",
    "\n",
    "image_files = [f for f in glob.iglob(f\"{batch_output_directory}/*\") if 'txt' not in f]\n",
    "\n",
    "missing_files = db.from_sequence(image_files, npartitions=8).map(process_file).compute()\n",
    "\n",
    "missing_files = [file for file in missing_files if file]\n",
    "\n",
    "if missing_files:\n",
    "    with open(\"missing_files.log\", \"w\") as log_file:\n",
    "        log_file.write(\"\\n\".join(missing_files))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Convert back to yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sahi.utils.coco import Coco\n",
    "\n",
    "for i in sorted(os.listdir(batch_output_directory)):\n",
    "  batch = os.path.join(batch_output_directory,i)\n",
    "    \n",
    "  sliced_dir = os.path.join(batch,\"sliced/\")\n",
    "  print(sliced_dir)\n",
    "  json_sliced = os.path.join(sliced_dir,\"sliced_coco.json\")\n",
    "  # init Coco object\n",
    "  coco = Coco.from_coco_dict_or_path(json_sliced, image_dir=sliced_dir)\n",
    "\n",
    "  # export converted YoloV5 formatted dataset into given output_dir with a 85% train/15% val split\n",
    "  coco.export_as_yolov5(\n",
    "    output_dir=os.path.join(source_directory,\"sliced_images_with_yolo_format\"),\n",
    "    train_split_rate=1,\n",
    "      disable_symlink=True\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Visualize sliced annotations on sliced images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(4, 5, figsize=(13,13))\n",
    "img_ind = 0\n",
    "for row_ind in range(4):\n",
    "    for column_ind in range(5):\n",
    "        # read image\n",
    "        img = Image.open(\"demo_data/sliced/\" + coco_dict[\"images\"][img_ind][\"file_name\"]).convert('RGBA')\n",
    "        # iterate over all annotations\n",
    "        for ann_ind in range(len(coco_dict[\"annotations\"])):\n",
    "            # find annotations that belong the selected image\n",
    "            if coco_dict[\"annotations\"][ann_ind][\"image_id\"] == coco_dict[\"images\"][img_ind][\"id\"]:\n",
    "                # convert coco bbox to pil bbox\n",
    "                xywh = coco_dict[\"annotations\"][ann_ind][\"bbox\"]\n",
    "                xyxy = [xywh[0], xywh[1], xywh[0]+xywh[2], xywh[1]+xywh[3]]\n",
    "                # visualize bbox over image\n",
    "                ImageDraw.Draw(img, 'RGBA').rectangle(xyxy, width=5)\n",
    "            axarr[row_ind, column_ind].imshow(img)\n",
    "        img_ind += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "super_model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
